{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T06:28:39.746050Z",
     "start_time": "2024-05-22T06:28:39.699593Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = np.load('data_lab5/data/training_sentences.npy', allow_pickle=True)\n",
    "train_labels = np.load('data_lab5/data/training_labels.npy', allow_pickle=True)\n",
    "test_data = np.load('data_lab5/data/test_sentences.npy', allow_pickle=True)\n",
    "test_labels = np.load('data_lab5/data/test_labels.npy', allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T06:01:55.827542Z",
     "start_time": "2024-05-22T06:01:55.808278Z"
    }
   },
   "id": "bd708df5ac61b3aa",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1\n",
    "def normalize_data(train_data, test_data, type=None):\n",
    "    if type is None:\n",
    "        return train_data, test_data\n",
    "    \n",
    "    elif type == 'standard':\n",
    "        mean_train_data = np.mean(train_data, axis=0)\n",
    "        std_train_data = np.std(train_data, axis=0)\n",
    "        normalized_train_data = np.divide(np.subtract(train_data, mean_train_data), std_train_data)\n",
    "        normalized_test_data = np.divide(np.subtract(test_data, mean_train_data), std_train_data)\n",
    "        return normalized_train_data, normalized_test_data\n",
    "    \n",
    "    elif type == 'l1':\n",
    "        train_norm = np.sum(np.abs(train_data), axis=0)\n",
    "        normalized_train_data = np.divide(train_data, train_norm)\n",
    "        test_norm = np.sum(np.abs(test_data), axis=0)\n",
    "        normalized_test_data = np.divide(test_data, test_norm)\n",
    "        return normalized_train_data, normalized_test_data\n",
    "    \n",
    "    elif type == 'l2':\n",
    "        train_norm = np.sqrt(np.sum(np.square(train_data), axis=0))\n",
    "        train_norm = np.add(train_norm, 1e-10)\n",
    "        normalized_train_data = np.divide(train_data, train_norm)\n",
    "        test_norm = np.sqrt(np.sum(np.square(test_data), axis=0))\n",
    "        test_norm = np.add(test_norm, 1e-10)\n",
    "        normalized_test_data = np.divide(test_data, test_norm)\n",
    "        return normalized_train_data, normalized_test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T06:19:48.790515Z",
     "start_time": "2024-05-22T06:19:48.783777Z"
    }
   },
   "id": "95d4db897c69fe94",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 2\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "    \n",
    "    def build_vocabulary(self, data):\n",
    "        idx = 0\n",
    "        words = []\n",
    "        for message in data:\n",
    "            for word in message:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = idx\n",
    "                    words.append(word)\n",
    "                    idx += 1\n",
    "                    \n",
    "    def get_features(self, data):\n",
    "        features = np.zeros((len(data), len(self.vocab)))\n",
    "        for i, message in enumerate(data):\n",
    "            for word in message:\n",
    "                if word in self.vocab:\n",
    "                    features[i, self.vocab[word]] += 1\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T06:19:49.525771Z",
     "start_time": "2024-05-22T06:19:49.520807Z"
    }
   },
   "id": "67eb2c12027e8af5",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9522\n"
     ]
    }
   ],
   "source": [
    "bag = BagOfWords()\n",
    "bag.build_vocabulary(train_data)\n",
    "train_features = bag.get_features(train_data)\n",
    "test_features = bag.get_features(test_data)\n",
    "normalized_train_features, normalized_test_features = normalize_data(train_features, test_features, 'l2')\n",
    "print(len(bag.vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T06:19:50.848279Z",
     "start_time": "2024-05-22T06:19:50.290196Z"
    }
   },
   "id": "dc846e7730037190",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9815217391304348\n",
      "F1: 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "svm_model = svm.SVC(kernel='linear', C=1)\n",
    "svm_model.fit(normalized_train_features, train_labels)\n",
    "test_predictions = svm_model.predict(normalized_test_features)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "print('Accuracy:', accuracy)\n",
    "print('F1:', f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T06:20:46.115301Z",
     "start_time": "2024-05-22T06:19:59.726004Z"
    }
   },
   "id": "51791270f4feaf8c",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  90  334  420 ... 8595  114 1355]]\n",
      "Most negative words:\n",
      "voicemail\n",
      "mobile\n",
      "08719181513\n",
      "urgent\n",
      "won\n",
      "Call\n",
      "08714712388\n",
      "84484\n",
      "ringtoneking\n",
      "08718738034\n",
      "Most positive words:\n",
      "me\n",
      "Im\n",
      "work\n",
      "class\n",
      "sir\n",
      "taken\n",
      "I\n",
      "him\n",
      "appreciate\n",
      "pick\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "coefficients = svm_model.coef_\n",
    "sorted_indices = np.argsort(coefficients)\n",
    "print(sorted_indices)\n",
    "    \n",
    "print('Most negative words:')\n",
    "for i in range(10):\n",
    "    index = sorted_indices[0, -i-1]\n",
    "    print(list(bag.vocab.keys())[index])\n",
    "    \n",
    "print('Most positive words:')\n",
    "for i in range(10):\n",
    "    index = sorted_indices[0, i]\n",
    "    print(list(bag.vocab.keys())[index])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T06:21:24.688305Z",
     "start_time": "2024-05-22T06:21:24.672697Z"
    }
   },
   "id": "d492bc6f136771f3",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m tfidf_vectorizer \u001B[38;5;241m=\u001B[39m TfidfVectorizer()\n\u001B[0;32m      3\u001B[0m train_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(message) \u001B[38;5;28;01mfor\u001B[39;00m message \u001B[38;5;129;01min\u001B[39;00m train_data]\n\u001B[1;32m----> 4\u001B[0m tfidf_train_features \u001B[38;5;241m=\u001B[39m \u001B[43mtfidf_vectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m test_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(message) \u001B[38;5;28;01mfor\u001B[39;00m message \u001B[38;5;129;01min\u001B[39;00m test_data]\n\u001B[0;32m      7\u001B[0m tfidf_test_features \u001B[38;5;241m=\u001B[39m tfidf_vectorizer\u001B[38;5;241m.\u001B[39mtransform(test_data)\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   2119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[0;32m   2120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[0;32m   2121\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[0;32m   2122\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[0;32m   2123\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[0;32m   2124\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[0;32m   2125\u001B[0m )\n\u001B[1;32m-> 2126\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[0;32m   2128\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[0;32m   2129\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[0;32m   1150\u001B[0m ):\n\u001B[1;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1375\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1376\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1377\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1378\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1379\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1380\u001B[0m             )\n\u001B[0;32m   1381\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1383\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1386\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1289\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1287\u001B[0m     vocabulary \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(vocabulary)\n\u001B[0;32m   1288\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m vocabulary:\n\u001B[1;32m-> 1289\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1290\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1291\u001B[0m         )\n\u001B[0;32m   1293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m indptr[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39miinfo(np\u001B[38;5;241m.\u001B[39mint32)\u001B[38;5;241m.\u001B[39mmax:  \u001B[38;5;66;03m# = 2**31 - 1\u001B[39;00m\n\u001B[0;32m   1294\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _IS_32BIT:\n",
      "\u001B[1;31mValueError\u001B[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_data = [' '.join(message) for message in train_data]\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_data)\n",
    "\n",
    "test_data = [' '.join(message) for message in test_data]\n",
    "tfidf_test_features = tfidf_vectorizer.transform(test_data)\n",
    "normalized_tfidf_train_features, normalized_tfidf_test_features = normalize_data(tfidf_train_features, tfidf_test_features, 'l2')\n",
    "\n",
    "svm_model.fit(normalized_tfidf_train_features, train_labels)\n",
    "test_predictions = svm_model.predict(normalized_tfidf_test_features)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('F1:', f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T06:35:02.620252Z",
     "start_time": "2024-05-22T06:35:02.549879Z"
    }
   },
   "id": "27dc3ed59315ae76",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ea87de8a0e82852"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
